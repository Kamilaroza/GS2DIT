{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Source: \n",
        "\n",
        "*   https://pypi.org/project/gpt-2-simple/#description\n",
        "*   https://medium.com/@stasinopoulos.dimitrios/a-beginners-guide-to-training-and-generating-text-using-gpt2-c2f2e1fbd10a\n",
        "*   https://colab.research.google.com/drive/1VLG8e7YSEwypxU-noRNhsv5dW4NfTGce#scrollTo=VHdTL8NDbAh3\n",
        "*  https://github.com/ak9250/gpt-2-colab\n",
        "*  https://www.aiweirdness.com/d-and-d-character-bios-now-making-19-03-15/\n",
        "*  https://minimaxir.com/2019/09/howto-gpt2/\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rgNM-NcAZ9aT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/zawemi/GS2DIT/blob/main/Class%203/gpt_2_shakespeare.ipynb#scrollTo=4tIUvFbLMUuE)"
      ],
      "metadata": {
        "id": "4tIUvFbLMUuE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Let's teach AI writing like a Shakespeare ðŸŽ“"
      ],
      "metadata": {
        "id": "MofLJqBHAWXI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Installing the model"
      ],
      "metadata": {
        "id": "W7wiPFGQQn9o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQACJ8lyUIR0",
        "outputId": "4b8bb3e3-5b75-4cd7-ff79-374e9a2be628"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gpt-2-simple\n",
            "  Downloading gpt_2_simple-0.8.1.tar.gz (26 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tensorflow>=2.5.1 in /usr/local/lib/python3.10/dist-packages (from gpt-2-simple) (2.12.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from gpt-2-simple) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from gpt-2-simple) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gpt-2-simple) (4.65.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from gpt-2-simple) (1.22.4)\n",
            "Collecting toposort (from gpt-2-simple)\n",
            "  Downloading toposort-1.10-py3-none-any.whl (8.5 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (23.3.3)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.54.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (3.8.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (0.4.8)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (16.0.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (23.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (2.12.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (0.32.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->gpt-2-simple) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->gpt-2-simple) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->gpt-2-simple) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->gpt-2-simple) (3.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow>=2.5.1->gpt-2-simple) (0.40.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.0.3 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow>=2.5.1->gpt-2-simple) (0.1.0)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow>=2.5.1->gpt-2-simple) (1.10.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (3.4.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (0.7.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (2.3.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (2.1.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (3.2.2)\n",
            "Building wheels for collected packages: gpt-2-simple\n",
            "  Building wheel for gpt-2-simple (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gpt-2-simple: filename=gpt_2_simple-0.8.1-py3-none-any.whl size=24559 sha256=5d509d78c16b77e874eb54d91b854773afcfdebe9acea3f78507001805acde82\n",
            "  Stored in directory: /root/.cache/pip/wheels/df/6a/fe/10d3223f78d1ac3e4c83bb4c5e2d28dfb1789c2fb4cc7ea8d0\n",
            "Successfully built gpt-2-simple\n",
            "Installing collected packages: toposort, gpt-2-simple\n",
            "Successfully installed gpt-2-simple-0.8.1 toposort-1.10\n"
          ]
        }
      ],
      "source": [
        "#install the library we'll use today\n",
        "!pip install gpt-2-simple"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Generating text with basic model"
      ],
      "metadata": {
        "id": "ADzeFwzaQ8cT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Importing and loading necessary components"
      ],
      "metadata": {
        "id": "d6Ah3D1CRK6o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import what we need\n",
        "import gpt_2_simple as gpt2 #for gpt-2 (our AI model)\n",
        "import os #lets us doing things with files and folders\n",
        "import requests #this one helps to dowload from the internet"
      ],
      "metadata": {
        "id": "mLg4pTPDaJJV"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#and let's download our AI model\n",
        "gpt2.download_gpt2()   # model is saved into current directory under /models/124M/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fIXHjaxvaWsV",
        "outputId": "be6ccf4b-d4a6-45fd-da54-a0c004031912"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching checkpoint: 1.05Mit [00:00, 797Mit/s]                                                      \n",
            "Fetching encoder.json: 1.05Mit [00:00, 5.73Mit/s]\n",
            "Fetching hparams.json: 1.05Mit [00:00, 194Mit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 498Mit [00:07, 66.0Mit/s]                                  \n",
            "Fetching model.ckpt.index: 1.05Mit [00:00, 319Mit/s]                                                \n",
            "Fetching model.ckpt.meta: 1.05Mit [00:00, 7.76Mit/s]\n",
            "Fetching vocab.bpe: 1.05Mit [00:00, 7.44Mit/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#strating the session so we can play with the gpt-2 model\n",
        "sess = gpt2.start_tf_sess()"
      ],
      "metadata": {
        "id": "6CCkn75KbBpg"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#we load the model from file to use it\n",
        "gpt2.load_gpt2(sess, run_name='124M', checkpoint_dir='models')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsBvHQsxZsyP",
        "outputId": "72e5d00c-6ce8-475a-f86d-88d2f05ef46a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading checkpoint models/124M/model.ckpt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Text generation"
      ],
      "metadata": {
        "id": "mDSFDj78RQJg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#this is how we would start model statement\n",
        "prefix = \"Is there a second Earth?\""
      ],
      "metadata": {
        "id": "-P5_fxZOgGlk"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#the model is generating text\n",
        "gpt2.generate(sess, run_name='124M', checkpoint_dir='models', prefix=prefix, length=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSYqTat0gNDo",
        "outputId": "b958ab9d-4da9-4cb2-9fe9-f85a1d41eed9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Is there a second Earth?\n",
            "\n",
            "The answer is, yes. And it's not as if there is.\n",
            "\n",
            "A third Earth is possible. It's possible that there might be a second Earth. But there are some things that are impossible â€“ and, of course,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Generating text with improved (finetuned) model"
      ],
      "metadata": {
        "id": "ML5helfmRjT0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IMPORTANT**\n",
        "</br>Restart the runtime (Runtime -> Restart runtime)"
      ],
      "metadata": {
        "id": "8cEaZKtRPx0S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Importing and loading necessary components"
      ],
      "metadata": {
        "id": "NIPDKskeR7i3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import what we need\n",
        "import gpt_2_simple as gpt2 #for gpt-2 (our AI model)\n",
        "import os #lets us doing things with files and folders\n",
        "import requests #this one helps to dowload from the internet"
      ],
      "metadata": {
        "id": "eHys5-bWPnhJ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get nietzsche texts\n",
        "!wget \"https://s3.amazonaws.com/text-datasets/nietzsche.txt\""
      ],
      "metadata": {
        "id": "dRTQyR7IqaOl",
        "outputId": "51f9ff97-287d-4c94-ab61-7c27b14fb7d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-21 15:19:03--  https://s3.amazonaws.com/text-datasets/nietzsche.txt\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.95.189, 54.231.169.144, 52.217.50.6, ...\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.95.189|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 600901 (587K) [text/plain]\n",
            "Saving to: â€˜nietzsche.txtâ€™\n",
            "\n",
            "nietzsche.txt       100%[===================>] 586.82K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2023-03-21 15:19:03 (4.21 MB/s) - â€˜nietzsche.txtâ€™ saved [600901/600901]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#game of thrones from https://www.kaggle.com/datasets/khulasasndh/game-of-thrones-books?select=001ssb.txt\n",
        "!gdown \"1CrL1wde_NGO68i5Prd_UNA_oW0cGQsxg&confirm=t\"\n",
        "!mv /content/001ssb.txt /content/got1.txt"
      ],
      "metadata": {
        "id": "pzDNTjJzuKDW",
        "outputId": "b8a3c87f-b2da-4507-e4b6-ba786e64353f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1CrL1wde_NGO68i5Prd_UNA_oW0cGQsxg&confirm=t\n",
            "To: /content/001ssb.txt\n",
            "\r  0% 0.00/1.63M [00:00<?, ?B/s]\r100% 1.63M/1.63M [00:00<00:00, 89.5MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#let's dowload a file with all Shakespeare plays\n",
        "!wget \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "!mv /content/input.txt /content/shakespeare.txt"
      ],
      "metadata": {
        "id": "9pwWGn5eqBJn",
        "outputId": "b59755af-4d8a-4592-c71a-bee603432b35",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-21 15:19:13--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: â€˜input.txtâ€™\n",
            "\n",
            "\rinput.txt             0%[                    ]       0  --.-KB/s               \rinput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2023-03-21 15:19:13 (19.9 MB/s) - â€˜input.txtâ€™ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#strating the session so we can play with the gpt-2 model\n",
        "sess = gpt2.start_tf_sess()"
      ],
      "metadata": {
        "id": "A0T2s8RxPnVr"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Teaching our model"
      ],
      "metadata": {
        "id": "bvllQvFxR9z6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#finetuning with shakespeare.txt (which, to be honest, means that we are teaching the model how to write like a shakespeare)\n",
        "#it takes a lot of time (~15min)...\n",
        "gpt2.finetune(sess, 'got1.txt', steps=500)   # steps is max number of training steps"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2RJetxF6UOfY",
        "outputId": "9b06d2f4-a01d-4d2d-8528-0df7e184bb76"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading checkpoint models/124M/model.ckpt\n",
            "Loading dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.74s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset has 433157 tokens\n",
            "Training...\n",
            "[1 | 6.24] loss=3.31 avg=3.31\n",
            "[2 | 8.31] loss=3.33 avg=3.32\n",
            "[3 | 10.40] loss=3.41 avg=3.35\n",
            "[4 | 12.49] loss=3.18 avg=3.31\n",
            "[5 | 14.57] loss=3.35 avg=3.32\n",
            "[6 | 16.67] loss=3.17 avg=3.29\n",
            "[7 | 18.77] loss=3.27 avg=3.29\n",
            "[8 | 20.95] loss=3.35 avg=3.30\n",
            "[9 | 23.12] loss=3.29 avg=3.29\n",
            "[10 | 25.23] loss=3.21 avg=3.29\n",
            "[11 | 27.35] loss=3.23 avg=3.28\n",
            "[12 | 29.47] loss=3.31 avg=3.28\n",
            "[13 | 31.59] loss=3.26 avg=3.28\n",
            "[14 | 33.74] loss=3.18 avg=3.27\n",
            "[15 | 35.87] loss=3.32 avg=3.28\n",
            "[16 | 38.01] loss=3.19 avg=3.27\n",
            "[17 | 40.16] loss=2.99 avg=3.25\n",
            "[18 | 42.30] loss=3.15 avg=3.25\n",
            "[19 | 44.45] loss=3.09 avg=3.24\n",
            "[20 | 46.62] loss=3.10 avg=3.23\n",
            "[21 | 48.77] loss=3.28 avg=3.23\n",
            "[22 | 50.93] loss=3.12 avg=3.23\n",
            "[23 | 53.11] loss=3.22 avg=3.23\n",
            "[24 | 55.28] loss=3.10 avg=3.22\n",
            "[25 | 57.46] loss=3.03 avg=3.21\n",
            "[26 | 59.67] loss=3.08 avg=3.21\n",
            "[27 | 61.85] loss=3.11 avg=3.20\n",
            "[28 | 64.04] loss=3.15 avg=3.20\n",
            "[29 | 66.23] loss=3.05 avg=3.19\n",
            "[30 | 68.42] loss=2.91 avg=3.18\n",
            "[31 | 70.64] loss=2.99 avg=3.18\n",
            "[32 | 72.84] loss=3.16 avg=3.18\n",
            "[33 | 75.05] loss=2.98 avg=3.17\n",
            "[34 | 77.26] loss=3.06 avg=3.16\n",
            "[35 | 79.48] loss=3.01 avg=3.16\n",
            "[36 | 81.70] loss=3.01 avg=3.15\n",
            "[37 | 83.92] loss=2.89 avg=3.15\n",
            "[38 | 86.14] loss=2.97 avg=3.14\n",
            "[39 | 88.36] loss=2.96 avg=3.14\n",
            "[40 | 90.59] loss=3.00 avg=3.13\n",
            "[41 | 92.83] loss=3.08 avg=3.13\n",
            "[42 | 95.06] loss=3.08 avg=3.13\n",
            "[43 | 97.31] loss=3.10 avg=3.13\n",
            "[44 | 99.56] loss=3.03 avg=3.12\n",
            "[45 | 101.81] loss=3.00 avg=3.12\n",
            "[46 | 104.06] loss=2.99 avg=3.12\n",
            "[47 | 106.33] loss=3.21 avg=3.12\n",
            "[48 | 108.61] loss=3.00 avg=3.12\n",
            "[49 | 110.88] loss=3.01 avg=3.11\n",
            "[50 | 113.17] loss=3.07 avg=3.11\n",
            "[51 | 115.46] loss=3.02 avg=3.11\n",
            "[52 | 117.75] loss=2.94 avg=3.11\n",
            "[53 | 120.04] loss=2.83 avg=3.10\n",
            "[54 | 122.34] loss=2.93 avg=3.10\n",
            "[55 | 124.65] loss=2.97 avg=3.09\n",
            "[56 | 126.96] loss=3.11 avg=3.09\n",
            "[57 | 129.27] loss=2.99 avg=3.09\n",
            "[58 | 131.58] loss=2.84 avg=3.09\n",
            "[59 | 133.89] loss=2.97 avg=3.08\n",
            "[60 | 136.21] loss=2.96 avg=3.08\n",
            "[61 | 138.51] loss=2.94 avg=3.08\n",
            "[62 | 140.81] loss=2.90 avg=3.07\n",
            "[63 | 143.10] loss=2.99 avg=3.07\n",
            "[64 | 145.40] loss=2.85 avg=3.07\n",
            "[65 | 147.69] loss=2.92 avg=3.06\n",
            "[66 | 149.97] loss=2.95 avg=3.06\n",
            "[67 | 152.25] loss=2.81 avg=3.06\n",
            "[68 | 154.53] loss=2.83 avg=3.05\n",
            "[69 | 156.81] loss=2.95 avg=3.05\n",
            "[70 | 159.09] loss=2.84 avg=3.05\n",
            "[71 | 161.35] loss=2.89 avg=3.04\n",
            "[72 | 163.62] loss=2.78 avg=3.04\n",
            "[73 | 165.89] loss=2.93 avg=3.04\n",
            "[74 | 168.16] loss=2.90 avg=3.03\n",
            "[75 | 170.44] loss=3.02 avg=3.03\n",
            "[76 | 172.70] loss=2.91 avg=3.03\n",
            "[77 | 174.97] loss=2.86 avg=3.03\n",
            "[78 | 177.23] loss=2.88 avg=3.02\n",
            "[79 | 179.50] loss=2.85 avg=3.02\n",
            "[80 | 181.77] loss=2.90 avg=3.02\n",
            "[81 | 184.04] loss=2.94 avg=3.02\n",
            "[82 | 186.31] loss=2.79 avg=3.01\n",
            "[83 | 188.58] loss=2.96 avg=3.01\n",
            "[84 | 190.86] loss=2.89 avg=3.01\n",
            "[85 | 193.14] loss=2.71 avg=3.00\n",
            "[86 | 195.43] loss=2.73 avg=3.00\n",
            "[87 | 197.72] loss=2.80 avg=3.00\n",
            "[88 | 200.01] loss=2.91 avg=3.00\n",
            "[89 | 202.29] loss=2.88 avg=2.99\n",
            "[90 | 204.57] loss=2.83 avg=2.99\n",
            "[91 | 206.87] loss=2.67 avg=2.99\n",
            "[92 | 209.17] loss=2.92 avg=2.98\n",
            "[93 | 211.46] loss=2.75 avg=2.98\n",
            "[94 | 213.75] loss=2.72 avg=2.98\n",
            "[95 | 216.05] loss=2.87 avg=2.97\n",
            "[96 | 218.35] loss=2.69 avg=2.97\n",
            "[97 | 220.65] loss=2.75 avg=2.97\n",
            "[98 | 222.94] loss=2.84 avg=2.96\n",
            "[99 | 225.23] loss=2.75 avg=2.96\n",
            "[100 | 227.53] loss=2.86 avg=2.96\n",
            "======== SAMPLE 1 ========\n",
            " rich, but not as a man of the West, the man I wanted to marry, the son of a whore, or the slave girl who tried to have both men. And no, I wanted to have only two. The Lord Commander had me with him, but he knew me no better. And I want one, and maybe one more. \"I want the blood of the dragon.\" He sounded almost as much like the singer, with the silver white eyes, as if someone had a hand that said, \"There is only one true dragon on the face of the earth. The wolf is the true wolf, the raven the true raven. Every one is his sister, and they all must die together. \"The wolf is the true wolf. The wolf is the true raven. \" The wolf is the true raven from the start. The wolf is the true raven. You have taken your curse off of Lady Frey. I would not risk that; that would not have been possible.\" \"No, as I said, with only . . . I would not risk it,\" she said, her cheeks red. \"You will die, Lord Commander. It will be that one of your men dies. If you have not yet found a dragon, we may begin a new one here, maybe. You will have a new wife and a new son, then you will know who the true dragon is and what that's about. And if you do not, I fear Lord Eddard would have you killed.\" \"I do not know the names of the three.\" \"If you want to know the names of the three,\" said the girl. \"That would be a great pity, my lord. I want Lady Frey with me. . . to kill him. I want him to die by me, and to love me with all my heart, knowing that his sister would be with him when the battle was done.\" \"There is no man stronger than the Lord Commander . . . to kill Lady Frey would be cruel and cruel to every man on earth. The Lord Commander's crimes, the crimes of his wives, have only made them worse every day, as I see it, by the day that comes. No one would have dared to say that Lord Frey would be too cruel for the gods, and yet I tremble, for me he seemed cruel at the time. He commanded that all who killed and wounded Lady Frey and those who lay their innocent lives before him be sent to the dungeons below the Wall. The gods would know. \n",
            "Lady Frey, you are truly my queen. The gods, I understand it. \"There are so many cruel things a king does. His queen is more cruel than all of them. There are so many children. A hundred of his own. You ought not be afraid. The king is not his own child, a mere shadow of him, but an heir, a mere child of his own. The father will kill, and the mother will take the burden. It is he who sits on your throne, you will see, and I promise you that I know that. In your own name, you shall do that, my lady.\" \n",
            "After that, she said nothing. \n",
            "The king rose. The queen stood at the door, hands clasped, while the queen's eyes were set on the face below, her face she could not have known was like that of the queen he had given him, her, the queen who died with him . . . \n",
            "\"No,\" said the king with a frown. \"The gods have forbidden the word of the king, and I am only a king without a King.\" Even so: \"I tell you, if you choose to speak the word of the king, I will tell you your message, and your duty. You must give me your commands, the lords, from time to time, until you are all clear and convincing, if I cannot, then only by \n",
            "promise.\" \n",
            "The king laughed. \"No more,\" he promised. \"My daughter will have her father's name and father's will be done as well. The last name is an honor. Let the queen do her duty. Let the king do her duty, and I ask she will do so with me, in my court. I do not want my wife to lose her husband, and I do not want her children to starve in the fields or be hunted down by wolves. I do not want my husband to be treated as a second-rate whore, to have his children killed and killed for the sake of his queen in the dungeons. I do not want a woman murdered by a king, or a queen taken from her husband. All this should be for a good cause. If I am not truly a queen then I am not truly a king. No one will be a king for long, if all is gone to the point of madness.\" \n",
            "\"My lady,\" said she. \"I am truly your champion here. If the gods will forgive me, I will take\n",
            "\n",
            "[101 | 242.01] loss=2.74 avg=2.96\n",
            "[102 | 244.32] loss=2.75 avg=2.95\n",
            "[103 | 246.60] loss=2.90 avg=2.95\n",
            "[104 | 248.89] loss=2.91 avg=2.95\n",
            "[105 | 251.17] loss=2.97 avg=2.95\n",
            "[106 | 253.46] loss=2.74 avg=2.95\n",
            "[107 | 255.74] loss=2.83 avg=2.95\n",
            "[108 | 258.03] loss=2.82 avg=2.94\n",
            "[109 | 260.31] loss=2.79 avg=2.94\n",
            "[110 | 262.60] loss=2.78 avg=2.94\n",
            "[111 | 264.88] loss=2.74 avg=2.94\n",
            "[112 | 267.18] loss=2.74 avg=2.93\n",
            "[113 | 269.47] loss=2.76 avg=2.93\n",
            "[114 | 271.75] loss=2.70 avg=2.93\n",
            "[115 | 274.03] loss=2.88 avg=2.93\n",
            "[116 | 276.31] loss=2.57 avg=2.92\n",
            "[117 | 278.60] loss=2.83 avg=2.92\n",
            "[118 | 280.89] loss=2.79 avg=2.92\n",
            "[119 | 283.17] loss=2.74 avg=2.92\n",
            "[120 | 285.46] loss=2.82 avg=2.91\n",
            "[121 | 287.75] loss=2.68 avg=2.91\n",
            "[122 | 290.04] loss=2.90 avg=2.91\n",
            "[123 | 292.33] loss=2.64 avg=2.91\n",
            "[124 | 294.62] loss=2.77 avg=2.91\n",
            "[125 | 296.91] loss=2.81 avg=2.90\n",
            "[126 | 299.20] loss=2.87 avg=2.90\n",
            "[127 | 301.49] loss=2.55 avg=2.90\n",
            "[128 | 303.79] loss=2.67 avg=2.90\n",
            "[129 | 306.09] loss=2.65 avg=2.89\n",
            "[130 | 308.37] loss=2.63 avg=2.89\n",
            "[131 | 310.67] loss=2.70 avg=2.89\n",
            "[132 | 312.96] loss=2.81 avg=2.89\n",
            "[133 | 315.25] loss=2.66 avg=2.88\n",
            "[134 | 317.55] loss=2.74 avg=2.88\n",
            "[135 | 319.84] loss=2.88 avg=2.88\n",
            "[136 | 322.13] loss=2.81 avg=2.88\n",
            "[137 | 324.43] loss=2.84 avg=2.88\n",
            "[138 | 326.72] loss=2.54 avg=2.87\n",
            "[139 | 329.01] loss=2.69 avg=2.87\n",
            "[140 | 331.31] loss=2.79 avg=2.87\n",
            "[141 | 333.60] loss=2.65 avg=2.87\n",
            "[142 | 335.89] loss=2.46 avg=2.86\n",
            "[143 | 338.18] loss=2.64 avg=2.86\n",
            "[144 | 340.48] loss=2.59 avg=2.86\n",
            "[145 | 342.78] loss=2.75 avg=2.85\n",
            "[146 | 345.07] loss=2.81 avg=2.85\n",
            "[147 | 347.36] loss=2.90 avg=2.85\n",
            "[148 | 349.66] loss=2.73 avg=2.85\n",
            "[149 | 351.95] loss=2.73 avg=2.85\n",
            "[150 | 354.26] loss=2.73 avg=2.85\n",
            "[151 | 356.55] loss=2.67 avg=2.85\n",
            "[152 | 358.85] loss=2.55 avg=2.84\n",
            "[153 | 361.14] loss=2.88 avg=2.84\n",
            "[154 | 363.43] loss=2.67 avg=2.84\n",
            "[155 | 365.73] loss=2.60 avg=2.84\n",
            "[156 | 368.02] loss=2.54 avg=2.84\n",
            "[157 | 370.31] loss=2.77 avg=2.83\n",
            "[158 | 372.61] loss=2.60 avg=2.83\n",
            "[159 | 374.91] loss=2.54 avg=2.83\n",
            "[160 | 377.20] loss=2.68 avg=2.83\n",
            "[161 | 379.50] loss=2.63 avg=2.82\n",
            "[162 | 381.79] loss=2.61 avg=2.82\n",
            "[163 | 384.08] loss=2.58 avg=2.82\n",
            "[164 | 386.37] loss=2.56 avg=2.81\n",
            "[165 | 388.67] loss=2.65 avg=2.81\n",
            "[166 | 390.96] loss=2.60 avg=2.81\n",
            "[167 | 393.25] loss=2.65 avg=2.81\n",
            "[168 | 395.55] loss=2.57 avg=2.81\n",
            "[169 | 397.84] loss=2.74 avg=2.80\n",
            "[170 | 400.13] loss=2.68 avg=2.80\n",
            "[171 | 402.43] loss=2.48 avg=2.80\n",
            "[172 | 404.72] loss=2.65 avg=2.80\n",
            "[173 | 407.01] loss=2.91 avg=2.80\n",
            "[174 | 409.30] loss=2.91 avg=2.80\n",
            "[175 | 411.59] loss=2.51 avg=2.80\n",
            "[176 | 413.88] loss=2.76 avg=2.80\n",
            "[177 | 416.18] loss=2.70 avg=2.79\n",
            "[178 | 418.47] loss=2.65 avg=2.79\n",
            "[179 | 420.75] loss=2.61 avg=2.79\n",
            "[180 | 423.04] loss=2.55 avg=2.79\n",
            "[181 | 425.33] loss=2.78 avg=2.79\n",
            "[182 | 427.62] loss=2.70 avg=2.79\n",
            "[183 | 429.91] loss=2.50 avg=2.78\n",
            "[184 | 432.21] loss=2.65 avg=2.78\n",
            "[185 | 434.50] loss=2.67 avg=2.78\n",
            "[186 | 436.79] loss=2.61 avg=2.78\n",
            "[187 | 439.08] loss=2.48 avg=2.78\n",
            "[188 | 441.37] loss=2.51 avg=2.77\n",
            "[189 | 443.66] loss=2.46 avg=2.77\n",
            "[190 | 445.95] loss=2.48 avg=2.77\n",
            "[191 | 448.24] loss=2.61 avg=2.76\n",
            "[192 | 450.53] loss=2.59 avg=2.76\n",
            "[193 | 452.83] loss=2.63 avg=2.76\n",
            "[194 | 455.11] loss=2.47 avg=2.76\n",
            "[195 | 457.40] loss=2.68 avg=2.76\n",
            "[196 | 459.69] loss=2.32 avg=2.75\n",
            "[197 | 461.97] loss=2.63 avg=2.75\n",
            "[198 | 464.27] loss=2.67 avg=2.75\n",
            "[199 | 466.55] loss=2.71 avg=2.75\n",
            "[200 | 468.84] loss=2.55 avg=2.75\n",
            "======== SAMPLE 1 ========\n",
            " crimson fur as it ripped off her cloak. \"This one is all black, boy, I've never seen \n",
            "one of these so before,\" she told her son. She had never seen him look so beautiful. He had gone as far as to dress \n",
            "him, as pale and taut as a moon, and his pale grey eyes found the cold heat of the deep in what he \n",
            "refused to understand. It was nothing more than a dream that haunted her, but she could smell it on her clothes, \n",
            "she would not find out what it was. \n",
            "The \n",
            "silver was as hard as it was to find. When the \n",
            "silver hit, it ripped a hole through the snow, where the cold pressed softly into \n",
            "the earth. The flakes were a warm white, and the snow was as heavy as a feather in her son's black hair, \n",
            "the snow was cold and thick and deep, so cold he could not use fire to melt it, so cold he could not \n",
            "hear the voices, or even hear them. He was so sick of his dreams, sick of what life and death might \n",
            "make him feel, so sick of them. When the silver hit, it ripped a hole through the snow. She had never \n",
            "heard a sound until her son said, a voice that she remembered, like a dead child, \"There must have been twenty of \n",
            "them,\" and she laughed. I'll have twenty, I'll have fifty, twenty thousand voices.\" \n",
            "Page 3\n",
            "\n",
            "She could hear his voice as she spun around, screaming in a fury. \"I don't have your voice, I won't hear you \n",
            "from this bed, I won't hear you here.\" \n",
            "\"They said there were forty thousand other people here, as well as two thousand thousand other men.\" \n",
            "\"Ah, that would be too many, and yet that's the truth of it, the others said.\" \n",
            "\"Seven thousand.\" \n",
            "\"Seven thousand, is that it? What was it?\" \n",
            "\"Eighteen thousand, and a hundred six thousand. It was fifty in the east, and forty thousand in the west, and forty in the west.\" \n",
            "\"You are right.\" \n",
            "\"How many thousand?\" \n",
            "\"One hundred.\" It was a hard call to make. \n",
            "\"A thousand,\" she said, \"and what do you mean? Four thousand? I mean, even fifty thousand would be too many, I \n",
            "would go to war with one man who had a name. Or, even the least, five thousand. That's why we sent you out here.\" \n",
            "He could take a whole world out of her, in that instant, all the women in the world, if she wanted. \n",
            "\"Why? Why would you send in such a few thousand?\" \n",
            "The silver had the power of a thousand gods. What made her think that, for some such grand purpose, any \n",
            "man had to be an imposter? Was she wrong? \n",
            "She had been told that the men and women of Winterfell were so large that no one should ever be more than \n",
            "two thousand. \"But how many had you sent?\" \n",
            "\"We sent thirty thousand, three thousand, eight thousand, and a thousand fifty, four thousand and three?\" her son \n",
            "repeated. \"No, no, no, and that was fifty, and that was three hundred thousand when the army was \n",
            "slight.\" \n",
            "\"What?\" \n",
            "\"The lords Stark, with the gods and the boys, five hundred and fifty thousand. We sent a thousand \n",
            "men, but even then-\" She was out of breath, out of mood, out of words. \n",
            "Her son's voice, his heavy breathing, as thick as ice, like a knife in a leaf, \n",
            "shaking his head from the side. \"Come now, child. This is all you ask. There must be a hundred thousand \n",
            "counting men here by then.\" \n",
            "Ser Rodrik opened his mouth to a response, but Mother only wanted his words to be heard. \n",
            "She had known his silence, her son's. He smiled warmly at the girl beside the fire. \"Your \n",
            "gods,\" he said. \"You've gone through so much, and you know that once again. You have the strength \n",
            "to tell me what you'd done.\" \n",
            "Maester Pycelle shook his head. \"I don't like the way my men look.\" He gave her a \n",
            "tired look. \"They're a dim view of things. I do have a father, no doubt about that. My children \n",
            "are no doubt \n",
            "beautiful, perhaps, their mothers are, but for three years you sent me away, as I am sure you did.\" \n",
            "\"You sent the Kingslayer off with the letter, the way you\n",
            "\n",
            "[201 | 482.12] loss=2.54 avg=2.74\n",
            "[202 | 484.40] loss=2.52 avg=2.74\n",
            "[203 | 486.68] loss=2.62 avg=2.74\n",
            "[204 | 488.97] loss=2.59 avg=2.74\n",
            "[205 | 491.26] loss=2.52 avg=2.74\n",
            "[206 | 493.54] loss=2.61 avg=2.73\n",
            "[207 | 495.82] loss=2.52 avg=2.73\n",
            "[208 | 498.12] loss=2.51 avg=2.73\n",
            "[209 | 500.40] loss=2.47 avg=2.73\n",
            "[210 | 502.70] loss=2.47 avg=2.72\n",
            "[211 | 504.99] loss=2.45 avg=2.72\n",
            "[212 | 507.27] loss=2.45 avg=2.72\n",
            "[213 | 509.56] loss=2.37 avg=2.71\n",
            "[214 | 511.85] loss=2.73 avg=2.71\n",
            "[215 | 514.14] loss=2.53 avg=2.71\n",
            "[216 | 516.43] loss=2.40 avg=2.71\n",
            "[217 | 518.72] loss=2.37 avg=2.70\n",
            "[218 | 521.01] loss=2.64 avg=2.70\n",
            "[219 | 523.30] loss=2.45 avg=2.70\n",
            "[220 | 525.59] loss=2.52 avg=2.70\n",
            "[221 | 527.87] loss=2.55 avg=2.70\n",
            "[222 | 530.16] loss=2.65 avg=2.70\n",
            "[223 | 532.45] loss=2.39 avg=2.69\n",
            "[224 | 534.74] loss=2.30 avg=2.69\n",
            "[225 | 537.03] loss=2.60 avg=2.69\n",
            "[226 | 539.31] loss=2.32 avg=2.68\n",
            "[227 | 541.60] loss=2.34 avg=2.68\n",
            "[228 | 543.88] loss=2.61 avg=2.68\n",
            "[229 | 546.17] loss=2.55 avg=2.68\n",
            "[230 | 548.46] loss=2.48 avg=2.67\n",
            "[231 | 550.76] loss=2.75 avg=2.68\n",
            "[232 | 553.04] loss=2.27 avg=2.67\n",
            "[233 | 555.32] loss=2.60 avg=2.67\n",
            "[234 | 557.62] loss=2.51 avg=2.67\n",
            "[235 | 559.90] loss=2.35 avg=2.66\n",
            "[236 | 562.19] loss=2.39 avg=2.66\n",
            "[237 | 564.48] loss=2.72 avg=2.66\n",
            "[238 | 566.76] loss=2.60 avg=2.66\n",
            "[239 | 569.05] loss=2.59 avg=2.66\n",
            "[240 | 571.34] loss=2.62 avg=2.66\n",
            "[241 | 573.63] loss=2.60 avg=2.66\n",
            "[242 | 575.92] loss=2.33 avg=2.66\n",
            "[243 | 578.21] loss=2.30 avg=2.65\n",
            "[244 | 580.50] loss=2.37 avg=2.65\n",
            "[245 | 582.78] loss=2.51 avg=2.65\n",
            "[246 | 585.07] loss=2.35 avg=2.64\n",
            "[247 | 587.36] loss=2.39 avg=2.64\n",
            "[248 | 589.65] loss=2.30 avg=2.64\n",
            "[249 | 591.94] loss=2.58 avg=2.64\n",
            "[250 | 594.23] loss=2.53 avg=2.64\n",
            "[251 | 596.51] loss=2.51 avg=2.63\n",
            "[252 | 598.80] loss=2.58 avg=2.63\n",
            "[253 | 601.09] loss=2.46 avg=2.63\n",
            "[254 | 603.38] loss=2.49 avg=2.63\n",
            "[255 | 605.66] loss=2.21 avg=2.63\n",
            "[256 | 607.95] loss=2.34 avg=2.62\n",
            "[257 | 610.24] loss=2.33 avg=2.62\n",
            "[258 | 612.54] loss=2.48 avg=2.62\n",
            "[259 | 614.83] loss=2.35 avg=2.62\n",
            "[260 | 617.12] loss=2.54 avg=2.61\n",
            "[261 | 619.41] loss=2.41 avg=2.61\n",
            "[262 | 621.70] loss=2.69 avg=2.61\n",
            "[263 | 624.00] loss=2.27 avg=2.61\n",
            "[264 | 626.28] loss=2.32 avg=2.61\n",
            "[265 | 628.57] loss=2.37 avg=2.60\n",
            "[266 | 630.85] loss=2.43 avg=2.60\n",
            "[267 | 633.14] loss=2.61 avg=2.60\n",
            "[268 | 635.43] loss=2.59 avg=2.60\n",
            "[269 | 637.72] loss=2.35 avg=2.60\n",
            "[270 | 640.01] loss=2.49 avg=2.60\n",
            "[271 | 642.30] loss=2.44 avg=2.60\n",
            "[272 | 644.59] loss=2.48 avg=2.60\n",
            "[273 | 646.88] loss=2.55 avg=2.59\n",
            "[274 | 649.19] loss=2.36 avg=2.59\n",
            "[275 | 651.47] loss=2.05 avg=2.59\n",
            "[276 | 653.76] loss=2.29 avg=2.58\n",
            "[277 | 656.06] loss=2.43 avg=2.58\n",
            "[278 | 658.35] loss=2.34 avg=2.58\n",
            "[279 | 660.64] loss=2.20 avg=2.57\n",
            "[280 | 662.93] loss=2.32 avg=2.57\n",
            "[281 | 665.21] loss=2.36 avg=2.57\n",
            "[282 | 667.50] loss=2.39 avg=2.57\n",
            "[283 | 669.79] loss=2.39 avg=2.57\n",
            "[284 | 672.09] loss=2.26 avg=2.56\n",
            "[285 | 674.38] loss=2.56 avg=2.56\n",
            "[286 | 676.67] loss=2.07 avg=2.56\n",
            "[287 | 678.95] loss=2.27 avg=2.55\n",
            "[288 | 681.25] loss=2.45 avg=2.55\n",
            "[289 | 683.54] loss=2.29 avg=2.55\n",
            "[290 | 685.82] loss=2.27 avg=2.55\n",
            "[291 | 688.11] loss=2.42 avg=2.55\n",
            "[292 | 690.40] loss=2.54 avg=2.55\n",
            "[293 | 692.69] loss=2.49 avg=2.55\n",
            "[294 | 694.98] loss=2.48 avg=2.55\n",
            "[295 | 697.27] loss=2.22 avg=2.54\n",
            "[296 | 699.56] loss=2.08 avg=2.54\n",
            "[297 | 701.85] loss=2.40 avg=2.54\n",
            "[298 | 704.15] loss=2.18 avg=2.53\n",
            "[299 | 706.43] loss=2.23 avg=2.53\n",
            "[300 | 708.73] loss=2.10 avg=2.52\n",
            "======== SAMPLE 1 ========\n",
            "\" The last one \n",
            "would have been dead or rotting, Jon thought, watching as the red-haired boy moved his mouth around \n",
            "the last of its red globules and the red blood filled his gaping mouth. He held out the gauntlet he'd gotten him, and it \n",
            "began to glow. It stopped and writhed like a sword in the cold light of the hall. \n",
            "\"Here too, boy,\" Robb said as the rest of the direwolves passed by, its gaunting white head \n",
            "spreading out from its massive silver eyes. \"Now, when the maester has been done with his inquiries, speak \n",
            "fast and openly. He'll tell you what he thinks of the whole thing.\" \n",
            "Jon felt a flinch as he stood there looking puzzled. What was happening here? Did it mean they were \n",
            "going to starve in King's Landing? Or would they be better off in the Trident? \n",
            "\"Gather us at once, then,\" Robb whispered to the direwolf. A hand wrapped around Jon's leg as the eunuch led \n",
            "him back to the bench beside the captain. \"I would prefer that you take my hand off as you are leading my \n",
            "direwolves.\" He turned and turned back into the direwolf again. The rest of the way down the tower, the maester \n",
            "remembered everything. The direwolves had been a part of the Night's Watch, the last bastion of peace before \n",
            "their time was done. The king had sent Joffrey and Bran into the castle to see what that good knight had \n",
            "done as well as any of his knights. He had been much more polite with Bran. If anything, he had seemed more \n",
            "patient in his instructions. \"A great deal of effort should be put into bringing down the maester this way, \n",
            "Jon,\" he heard himself saying. \n",
            "Robb did not deny it. \"Joffrey told me this morning that this way, as you will.\" He made a gesture to Jon. \"You \n",
            "will go to me on the morrow, my lord, and say that you have the answers you seek.\" \n",
            "\"Aye,\" he said with a bitter smile. \"Yet no more, my lord. The words you seek need not concern your son, \n",
            "they concern him that much.\" \n",
            "\"The maester requires the help of men of his judgment. You have an appetite for adventure.\" \n",
            "\"The maester will reward you as you please.\" The direwolf nodded. His jaws dropped as he looked at Jon, \n",
            "Page 127\n",
            "\n",
            "stunned, but he smiled nonetheless and moved on. \n",
            "Jon sat cross-legged on the ground, eyes closed. The eunuch lifted his arm and began to chew on the top of the \n",
            "head. \"Wipe that filthy face off with a damp cloth,\" he commanded Ser Barristan Selmy, the Lord Steward \n",
            "of the Hall. \n",
            "The poor wretch began to scrape away at the scalp with his thumb, until the thin crusting of flesh \n",
            "covered where his fingers had been dicing for hours that night. He spat with his teeth and spat again. \"Here,\" he \n",
            "puffed, \"here, where, where, where.\" \n",
            "Ser Barristan's head snapped around on side to side, a gasp came from all around him. The eunuch snorted, \n",
            "and began to lick at the meat again and again, until the thick crusting of flesh left nothing visible but \n",
            "his fingers. When the rest of the body was gone, Ser Barristan looked around nervously to see if his \n",
            "master-at-arms had left as he laid the carcass. \n",
            "And there he was, gaping, the light grey slouching corpse of a lion. Looking around at all the people, and \n",
            "saying no one heard him ask what they were doing. \n",
            "Ser Barristan groaned in delight. \"This might be my last meal here,\" he whispered to himself as he laid the \n",
            "wretched carcass in his lap. He would have to go back to his room to find Robert gone. \n",
            "\"I know,\" said Ser Brynden, still groggy. \"You and Jory and I will be there again.\" \n",
            "He turned to look over to where the eunuch was kneeling, with Father and the Grand Maester. \"I hope the \n",
            "children will be as pleasant to children as you and Jory and the others have become.\" \n",
            "The small lord nodded. \"The servants will take their leave of us.\" \n",
            "Ser Brynden lowered the corpse to Jon's side. Father followed shortly afterward, and Jon followed soon after. \n",
            "Page 128\n",
            "\n",
            "\"Father,\" he said as his lord father followed him with the black-toed men around them.\n",
            "\n",
            "[301 | 721.80] loss=2.55 avg=2.52\n",
            "[302 | 724.08] loss=2.28 avg=2.52\n",
            "[303 | 726.37] loss=2.30 avg=2.52\n",
            "[304 | 728.66] loss=2.40 avg=2.52\n",
            "[305 | 730.95] loss=2.40 avg=2.52\n",
            "[306 | 733.24] loss=2.44 avg=2.52\n",
            "[307 | 735.53] loss=2.36 avg=2.51\n",
            "[308 | 737.81] loss=2.22 avg=2.51\n",
            "[309 | 740.10] loss=2.30 avg=2.51\n",
            "[310 | 742.38] loss=2.30 avg=2.51\n",
            "[311 | 744.67] loss=2.31 avg=2.51\n",
            "[312 | 746.96] loss=2.19 avg=2.50\n",
            "[313 | 749.24] loss=2.27 avg=2.50\n",
            "[314 | 751.53] loss=2.23 avg=2.50\n",
            "[315 | 753.81] loss=2.35 avg=2.49\n",
            "[316 | 756.10] loss=2.39 avg=2.49\n",
            "[317 | 758.38] loss=2.22 avg=2.49\n",
            "[318 | 760.66] loss=2.38 avg=2.49\n",
            "[319 | 762.95] loss=2.46 avg=2.49\n",
            "[320 | 765.23] loss=2.33 avg=2.49\n",
            "[321 | 767.52] loss=2.52 avg=2.49\n",
            "[322 | 769.82] loss=2.26 avg=2.49\n",
            "[323 | 772.10] loss=2.25 avg=2.48\n",
            "[324 | 774.39] loss=2.21 avg=2.48\n",
            "[325 | 776.67] loss=2.14 avg=2.48\n",
            "[326 | 778.96] loss=1.95 avg=2.47\n",
            "[327 | 781.25] loss=2.06 avg=2.47\n",
            "[328 | 783.55] loss=2.09 avg=2.46\n",
            "[329 | 785.83] loss=2.33 avg=2.46\n",
            "[330 | 788.12] loss=2.16 avg=2.46\n",
            "[331 | 790.41] loss=2.51 avg=2.46\n",
            "[332 | 792.69] loss=2.00 avg=2.45\n",
            "[333 | 794.99] loss=2.40 avg=2.45\n",
            "[334 | 797.27] loss=2.33 avg=2.45\n",
            "[335 | 799.57] loss=2.38 avg=2.45\n",
            "[336 | 801.85] loss=2.04 avg=2.45\n",
            "[337 | 804.14] loss=2.13 avg=2.44\n",
            "[338 | 806.43] loss=2.07 avg=2.44\n",
            "[339 | 808.72] loss=2.10 avg=2.44\n",
            "[340 | 811.01] loss=2.15 avg=2.43\n",
            "[341 | 813.30] loss=2.26 avg=2.43\n",
            "[342 | 815.59] loss=2.31 avg=2.43\n",
            "[343 | 817.88] loss=2.36 avg=2.43\n",
            "[344 | 820.18] loss=1.91 avg=2.42\n",
            "[345 | 822.47] loss=2.23 avg=2.42\n",
            "[346 | 824.76] loss=1.98 avg=2.42\n",
            "[347 | 827.05] loss=2.14 avg=2.42\n",
            "[348 | 829.34] loss=2.20 avg=2.41\n",
            "[349 | 831.64] loss=2.19 avg=2.41\n",
            "[350 | 833.93] loss=2.19 avg=2.41\n",
            "[351 | 836.23] loss=2.10 avg=2.41\n",
            "[352 | 838.51] loss=2.13 avg=2.40\n",
            "[353 | 840.80] loss=2.21 avg=2.40\n",
            "[354 | 843.11] loss=2.15 avg=2.40\n",
            "[355 | 845.40] loss=2.23 avg=2.40\n",
            "[356 | 847.68] loss=1.95 avg=2.39\n",
            "[357 | 849.98] loss=2.24 avg=2.39\n",
            "[358 | 852.28] loss=2.12 avg=2.39\n",
            "[359 | 854.58] loss=1.94 avg=2.38\n",
            "[360 | 856.87] loss=2.38 avg=2.38\n",
            "[361 | 859.16] loss=2.20 avg=2.38\n",
            "[362 | 861.46] loss=2.41 avg=2.38\n",
            "[363 | 863.76] loss=2.08 avg=2.38\n",
            "[364 | 866.05] loss=2.14 avg=2.38\n",
            "[365 | 868.36] loss=2.20 avg=2.37\n",
            "[366 | 870.65] loss=2.19 avg=2.37\n",
            "[367 | 872.95] loss=2.27 avg=2.37\n",
            "[368 | 875.24] loss=2.19 avg=2.37\n",
            "[369 | 877.53] loss=2.03 avg=2.37\n",
            "[370 | 879.83] loss=2.39 avg=2.37\n",
            "[371 | 882.13] loss=2.12 avg=2.36\n",
            "[372 | 884.43] loss=2.22 avg=2.36\n",
            "[373 | 886.72] loss=2.12 avg=2.36\n",
            "[374 | 889.01] loss=2.19 avg=2.36\n",
            "[375 | 891.31] loss=2.08 avg=2.35\n",
            "[376 | 893.62] loss=2.20 avg=2.35\n",
            "[377 | 895.91] loss=2.47 avg=2.35\n",
            "[378 | 898.19] loss=1.97 avg=2.35\n",
            "[379 | 900.48] loss=2.19 avg=2.35\n",
            "[380 | 902.77] loss=2.06 avg=2.35\n",
            "[381 | 905.07] loss=2.01 avg=2.34\n",
            "[382 | 907.37] loss=2.26 avg=2.34\n",
            "[383 | 909.66] loss=2.12 avg=2.34\n",
            "[384 | 911.95] loss=2.50 avg=2.34\n",
            "[385 | 914.26] loss=1.85 avg=2.34\n",
            "[386 | 916.60] loss=1.96 avg=2.33\n",
            "[387 | 918.94] loss=1.98 avg=2.33\n",
            "[388 | 921.23] loss=2.10 avg=2.33\n",
            "[389 | 923.52] loss=1.93 avg=2.32\n",
            "[390 | 925.81] loss=2.36 avg=2.32\n",
            "[391 | 928.11] loss=2.24 avg=2.32\n",
            "[392 | 930.41] loss=1.92 avg=2.32\n",
            "[393 | 932.70] loss=1.76 avg=2.31\n",
            "[394 | 934.99] loss=2.27 avg=2.31\n",
            "[395 | 937.28] loss=1.88 avg=2.31\n",
            "[396 | 939.57] loss=1.84 avg=2.30\n",
            "[397 | 941.88] loss=2.15 avg=2.30\n",
            "[398 | 944.18] loss=2.06 avg=2.30\n",
            "[399 | 946.47] loss=2.25 avg=2.30\n",
            "[400 | 948.76] loss=1.86 avg=2.29\n",
            "======== SAMPLE 1 ========\n",
            " the queen gave her head. He looked at her strangely, but she loved \n",
            "that. \n",
            "\"Who is she?\" Bran asked in a voice so sudden that it almost seemed to be a curse. The guardsman in the \n",
            "gown \n",
            "whispered, \"She's my sister. No one knows her better.\" \n",
            "\"What about her?\" Bran asked. \"Are you sure? Is your brother Snow too?\" \n",
            "\"Who is she?\" the guard asked in sharp sharp contrast. \n",
            "Bran looked around the yard, and suddenly he saw the castle where he had found her, a vast emptiness of dark \n",
            "water that only shadows knew bounds. He was no longer certain what he saw. \n",
            "He saw a long white hallway as straight as its winding twist. Thick black lines crept past his eyes, \n",
            "and that was it. He groped for his dagger as a cold wind blew across a flat blue sky. Then it \n",
            "was gone. \n",
            "The thought of Bran watching her was something he had not felt before. That cold air was very strange, \n",
            "even to him; it made everything seem as though it were coming at him from all sides. \n",
            "And it would come at him, until it finally came at Bran himself. With a terrible dream. \n",
            "The great white hand held Bran in both arms. He knelt, covered his head with his hands, and hugged \n",
            "himself. \n",
            "Page 348\n",
            "\n",
            ". . . \n",
            "The guardsman who was with Bran asked, \"What has happened to Bran?\" \n",
            "Bran frowned, wondering whether it was true or false. The first time he had ever seen a woman, he thought, \n",
            "it had not felt so real. He would have remembered the day. \n",
            "Another time his senses had been blind. He put his arm around his back and pulled himself toward the wall. \n",
            "Bran pushed himself back on the sill and tried to follow. His legs crunched into him like a crab's, and he \n",
            "fell. He tried \n",
            ". . . and finally he was fighting with his fingers under his arm. \n",
            "There was a knock at the door. Bran looked up, and there was a knock no knocking. \n",
            "Bran held his hand. There was a knock again. \n",
            "Again. The door opened. Bran pushed himself off and began to climb up. The guardsman pushed him \n",
            "away. Bran looked down. The floor was wet with blood. \n",
            "Everything, all of it, except for the smell. He could smell the wetness in his pants. He groped for his sword. \n",
            "Bran swung it toward the open wall. The other guardsmen ran to him, but he was too frightened to get up, \n",
            "knowing that it might not last long. If he ran, they would be mercifully more forthcoming . . . and that was \n",
            "when he plunged. His sword was found a foot from the door. When he tried to pull it free, it raked him in. \n",
            "Bran's sword hiked up in the air and knocked him across the face before he could even hope to make a sound. \n",
            "He fell. \n",
            "There was no more to say. \n",
            "The footsteps were louder. As he tried to reach the bed, he heard a raucous ovation. Bran found his feet \n",
            "swim, stumbled, and slipped. \n",
            "Arya Stark's bed, a vast closet under the door. From the sound it made, the closet was more \n",
            "than Arya could have imagined. It had to be Sansa's room. \n",
            "Arya Stark came snuggling in the tightest of tresses, her hands clasped under her arms, her fingers tracing the \n",
            "wood under her breath. She had a huge warm bath, and she could do with a bath that was pretty and \n",
            "cold. The smell of it made Arya feel strange and unhinged. But no, she would not have liked \n",
            "being squeezed between them, not when they were so close. They could still go down together, they could still go \n",
            "up against each other, right at the beginning of everything. They had almost been brothers, half sisters, in the \n",
            "names of things that had happened in their life . . . \n",
            "\"Arya,\" Sansa said as a pale hand moved over her bedside table. \"My god . . .\" \n",
            "\"My sweet baby,\" Arya said eagerly, not so much for her bed but because the Lord of the Eyrie was sitting by their \n",
            "bed, and so she said nothing. She loved the way he made her feel. It made her feel a thousand \n",
            "different ways. He was hugging her inside and out, kissing her under her arms, whispering to her inside and her \n",
            "outside, all in a dream. \n",
            "That dream\n",
            "\n",
            "[401 | 962.25] loss=2.20 avg=2.29\n",
            "[402 | 964.53] loss=2.06 avg=2.29\n",
            "[403 | 966.82] loss=2.05 avg=2.29\n",
            "[404 | 969.11] loss=2.15 avg=2.29\n",
            "[405 | 971.39] loss=2.09 avg=2.28\n",
            "[406 | 973.67] loss=2.13 avg=2.28\n",
            "[407 | 975.95] loss=1.89 avg=2.28\n",
            "[408 | 978.24] loss=2.06 avg=2.28\n",
            "[409 | 980.53] loss=1.80 avg=2.27\n",
            "[410 | 982.81] loss=1.96 avg=2.27\n",
            "[411 | 985.10] loss=1.80 avg=2.26\n",
            "[412 | 987.39] loss=2.06 avg=2.26\n",
            "[413 | 989.68] loss=2.14 avg=2.26\n",
            "[414 | 991.97] loss=1.74 avg=2.26\n",
            "[415 | 994.26] loss=1.74 avg=2.25\n",
            "[416 | 996.56] loss=2.27 avg=2.25\n",
            "[417 | 998.86] loss=1.94 avg=2.25\n",
            "[418 | 1001.16] loss=1.90 avg=2.24\n",
            "[419 | 1003.45] loss=1.95 avg=2.24\n",
            "[420 | 1005.75] loss=2.14 avg=2.24\n",
            "[421 | 1008.05] loss=2.27 avg=2.24\n",
            "[422 | 1010.35] loss=1.98 avg=2.24\n",
            "[423 | 1012.65] loss=1.79 avg=2.23\n",
            "[424 | 1014.95] loss=2.15 avg=2.23\n",
            "[425 | 1017.25] loss=1.71 avg=2.23\n",
            "[426 | 1019.55] loss=1.68 avg=2.22\n",
            "[427 | 1021.85] loss=1.91 avg=2.22\n",
            "[428 | 1024.14] loss=1.86 avg=2.21\n",
            "[429 | 1026.44] loss=1.51 avg=2.21\n",
            "[430 | 1028.74] loss=2.18 avg=2.21\n",
            "[431 | 1031.04] loss=1.75 avg=2.20\n",
            "[432 | 1033.34] loss=2.18 avg=2.20\n",
            "[433 | 1035.62] loss=2.14 avg=2.20\n",
            "[434 | 1037.92] loss=2.19 avg=2.20\n",
            "[435 | 1040.21] loss=1.95 avg=2.20\n",
            "[436 | 1042.51] loss=1.49 avg=2.19\n",
            "[437 | 1044.80] loss=1.90 avg=2.19\n",
            "[438 | 1047.09] loss=1.98 avg=2.19\n",
            "[439 | 1049.37] loss=2.04 avg=2.18\n",
            "[440 | 1051.66] loss=2.05 avg=2.18\n",
            "[441 | 1053.97] loss=1.61 avg=2.18\n",
            "[442 | 1056.26] loss=1.85 avg=2.17\n",
            "[443 | 1058.55] loss=2.18 avg=2.17\n",
            "[444 | 1060.83] loss=1.88 avg=2.17\n",
            "[445 | 1063.12] loss=1.68 avg=2.17\n",
            "[446 | 1065.41] loss=1.75 avg=2.16\n",
            "[447 | 1067.70] loss=1.69 avg=2.16\n",
            "[448 | 1069.98] loss=1.78 avg=2.15\n",
            "[449 | 1072.26] loss=1.95 avg=2.15\n",
            "[450 | 1074.55] loss=1.91 avg=2.15\n",
            "[451 | 1076.84] loss=1.88 avg=2.15\n",
            "[452 | 1079.13] loss=2.17 avg=2.15\n",
            "[453 | 1081.41] loss=1.76 avg=2.14\n",
            "[454 | 1083.69] loss=2.05 avg=2.14\n",
            "[455 | 1085.98] loss=1.80 avg=2.14\n",
            "[456 | 1088.27] loss=2.24 avg=2.14\n",
            "[457 | 1090.56] loss=1.75 avg=2.14\n",
            "[458 | 1092.85] loss=1.89 avg=2.13\n",
            "[459 | 1095.13] loss=1.39 avg=2.13\n",
            "[460 | 1097.42] loss=1.89 avg=2.12\n",
            "[461 | 1099.70] loss=1.69 avg=2.12\n",
            "[462 | 1101.99] loss=1.77 avg=2.12\n",
            "[463 | 1104.28] loss=1.98 avg=2.11\n",
            "[464 | 1106.57] loss=1.44 avg=2.11\n",
            "[465 | 1108.86] loss=1.66 avg=2.10\n",
            "[466 | 1111.15] loss=2.19 avg=2.10\n",
            "[467 | 1113.44] loss=1.87 avg=2.10\n",
            "[468 | 1115.74] loss=1.73 avg=2.10\n",
            "[469 | 1118.03] loss=1.42 avg=2.09\n",
            "[470 | 1120.32] loss=2.04 avg=2.09\n",
            "[471 | 1122.61] loss=1.65 avg=2.09\n",
            "[472 | 1124.91] loss=1.85 avg=2.08\n",
            "[473 | 1127.21] loss=1.61 avg=2.08\n",
            "[474 | 1129.50] loss=1.60 avg=2.07\n",
            "[475 | 1131.83] loss=1.80 avg=2.07\n",
            "[476 | 1134.13] loss=2.10 avg=2.07\n",
            "[477 | 1136.44] loss=2.24 avg=2.07\n",
            "[478 | 1138.77] loss=1.91 avg=2.07\n",
            "[479 | 1141.07] loss=1.95 avg=2.07\n",
            "[480 | 1143.36] loss=1.92 avg=2.07\n",
            "[481 | 1145.64] loss=1.78 avg=2.07\n",
            "[482 | 1147.93] loss=2.20 avg=2.07\n",
            "[483 | 1150.23] loss=1.60 avg=2.06\n",
            "[484 | 1152.52] loss=1.72 avg=2.06\n",
            "[485 | 1154.82] loss=1.92 avg=2.06\n",
            "[486 | 1157.12] loss=1.67 avg=2.05\n",
            "[487 | 1159.40] loss=1.77 avg=2.05\n",
            "[488 | 1161.69] loss=1.64 avg=2.05\n",
            "[489 | 1163.99] loss=2.31 avg=2.05\n",
            "[490 | 1166.28] loss=1.97 avg=2.05\n",
            "[491 | 1168.57] loss=1.96 avg=2.05\n",
            "[492 | 1170.86] loss=1.85 avg=2.05\n",
            "[493 | 1173.14] loss=1.67 avg=2.04\n",
            "[494 | 1175.44] loss=1.58 avg=2.04\n",
            "[495 | 1177.73] loss=1.62 avg=2.03\n",
            "[496 | 1180.02] loss=1.79 avg=2.03\n",
            "[497 | 1182.31] loss=1.41 avg=2.02\n",
            "[498 | 1184.59] loss=1.60 avg=2.02\n",
            "[499 | 1186.88] loss=2.00 avg=2.02\n",
            "[500 | 1189.21] loss=1.79 avg=2.02\n",
            "Saving checkpoint/run1/model-500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Text generation"
      ],
      "metadata": {
        "id": "bUagiJzBTeoL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prefix = \"What happened to Arya?\""
      ],
      "metadata": {
        "id": "qzTK7bdIPeOY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpt2.generate(sess, prefix=prefix, length=150)"
      ],
      "metadata": {
        "id": "ZCaaNXR7kI9I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57c0c228-6092-47de-b3d3-680462771ab8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What happened to Arya? I wonder.\" \n",
            "\"That was a stupid question, Robb,\" said Sansa, \"but I have questions too. You know the ones you want answered.\" \n",
            "Robb turned his head, but Sansa could see that he was looking at her strangely. She tried to tell him to go away, but he \n",
            "didn't hear her. He loped her, and Sansa scolded him. \n",
            "\"You are meant for this. If you ever look at me, you'll see that I am a woman of the Night's Watch.\" \n",
            "\"I'll tell you. You won't like that. Please, don't tell me you chose me for myself, not for myself.\" \n",
            "\"I would never\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Saving model to Google Drive (optional)"
      ],
      "metadata": {
        "id": "zlM6aQYZSccl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYXmOFl5Bjhv",
        "outputId": "564ebb74-2ba5-403f-dc4b-c36ce1478d52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpt2.copy_checkpoint_to_gdrive(run_name='run1')"
      ],
      "metadata": {
        "id": "3RUjr4_ZluKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can find more texts e.g. on:\n",
        "https://www.gutenberg.org/cache/epub/1597/pg1597.txt\n",
        "</br></br>\n",
        "You can download them to Colab using code similar to the ones below."
      ],
      "metadata": {
        "id": "OUhaGg_uS6o5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!wget https://www.gutenberg.org/cache/epub/1597/pg1597.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7K9X3K8TEwj",
        "outputId": "d0760c42-a0e4-4dcf-b7cc-ca98aaffa2b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-21 14:49:16--  https://www.gutenberg.org/cache/epub/1597/pg1597.txt\n",
            "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 329071 (321K) [text/plain]\n",
            "Saving to: â€˜pg1597.txtâ€™\n",
            "\n",
            "pg1597.txt          100%[===================>] 321.36K   800KB/s    in 0.4s    \n",
            "\n",
            "2023-03-21 14:49:22 (800 KB/s) - â€˜pg1597.txtâ€™ saved [329071/329071]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!wget https://www.gutenberg.org/files/98/98-0.txt"
      ],
      "metadata": {
        "id": "HYL0wij2m4Gf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42bf360b-ce90-4a36-d434-44820124b877"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-02-22 13:25:10--  https://www.gutenberg.org/files/98/98-0.txt\n",
            "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 807231 (788K) [text/plain]\n",
            "Saving to: â€˜98-0.txtâ€™\n",
            "\n",
            "98-0.txt            100%[===================>] 788.31K   718KB/s    in 1.1s    \n",
            "\n",
            "2023-02-22 13:25:12 (718 KB/s) - â€˜98-0.txtâ€™ saved [807231/807231]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#https://github.com/matt-dray/tng-stardate/tree/master/data/scripts"
      ],
      "metadata": {
        "id": "VClsbkgRxYvR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}